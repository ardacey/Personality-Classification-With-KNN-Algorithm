{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"16P.csv\", encoding='cp1252')\n",
    "del data[data.columns[0]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part I get data files from the same directory as the .ipynb file. I used \"cp1252\" for encoding and delete first part for unnecessary \"response id\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti_codes = ['ESTJ', 'ENTJ', 'ESFJ', 'ENFJ', 'ISTJ', 'ISFJ', 'INTJ', 'INFJ', 'ESTP', 'ESFP', 'ENTP', 'ENFP', 'ISTP', 'ISFP', 'INTP', 'INFP']\n",
    "numbers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "mbti_dict = dict(zip(mbti_codes, numbers))\n",
    "matrix = {number: {\"TP\": 0, \"FP\": 0, \"FN\": 0, \"TN\": 0} for number in numbers}\n",
    "data[data.columns[-1]] = data[data.columns[-1]].apply(lambda x: mbti_dict[x])\n",
    "data_array = data.to_numpy()\n",
    "dependent_variable = data_array[:, -1]\n",
    "independent_variable = data_array[:, :-1]\n",
    "cross_code = 0\n",
    "data_norm = (independent_variable - np.min(independent_variable))/(np.max(independent_variable) - np.min(independent_variable))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part I prepare the important data arrays and encoding the mbti personalities to numbers. \"matrix\" dictionary holds the confusion matrix for all datas and personalities. \"data\" dictionary holds datas and their corresponding personalities. After I created the \"data dictionary\", i make it numpy array and split the answer datas to independent_variable and corresponding personalities to dependent_variable. At the bottom part I created cross_code variable for count the number in the 5-fold cross validation. I alsÄ± created data_norm for make normalization and use the normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(neighbor_size, test):\n",
    "    distances = list()\n",
    "    dist = np.linalg.norm((test - train_data), axis=1)\n",
    "    distances = np.column_stack((train_data, dist))\n",
    "    distances = distances[distances[:, -1].argsort()]\n",
    "    neighbor = list()\n",
    "    for j in range(neighbor_size):\n",
    "        neighbor.append(distances[j][:-1])\n",
    "    return neighbor\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main function of k-nearest neighbor algorithm. This function takes 1 data from independent variables and calculate the distance between all train values. For this, I used linalg.norm() function. It simply calculates the Euclidean distance between 2 points in vector form. Actually I need a for loop for calculate the distances between test and train_data for every train data, but i take train_data as 1 dimensional array and calculate the distance with this 1 dimensional array. With this, I got rid of 1 loop and made the algorithm faster. After calculation of distance, i sort all distances and take the nearest \"k\" distances according to the value of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected test data: 1 out of 5\n",
      "With Normalization:\n",
      "\n",
      "Neighbor size:  1\n",
      "Correct Answers:  11747\n",
      "Wrong Answers:  253\n",
      "Accuracy: %97.89166666666667\n",
      "Precision: %76.55786350148368\n",
      "Recall: %97.9746835443038\n",
      "\n",
      "Neighbor size:  3\n",
      "Correct Answers:  11864\n",
      "Wrong Answers:  136\n",
      "Accuracy: %98.37916666666666\n",
      "Precision: %81.27282211789255\n",
      "Recall: %98.11083123425692\n",
      "\n",
      "Neighbor size:  5\n",
      "Correct Answers:  11873\n",
      "Wrong Answers:  127\n",
      "Accuracy: %98.56666666666666\n",
      "Precision: %83.14407381121363\n",
      "Recall: %98.28020134228188\n",
      "\n",
      "Neighbor size:  7\n",
      "Correct Answers:  11876\n",
      "Wrong Answers:  124\n",
      "Accuracy: %98.66666666666667\n",
      "Precision: %84.17653390742734\n",
      "Recall: %98.36477987421384\n",
      "\n",
      "Neighbor size:  9\n",
      "Correct Answers:  11877\n",
      "Wrong Answers:  123\n",
      "Accuracy: %98.72833333333332\n",
      "Precision: %84.82549317147192\n",
      "Recall: %98.41549295774648\n",
      "\n",
      "Without Normalization:\n",
      "\n",
      "Neighbor size:  1\n",
      "Correct Answers:  11744\n",
      "Wrong Answers:  256\n",
      "Accuracy: %98.58472222222223\n",
      "Precision: %83.29779673063256\n",
      "Recall: %98.34277323264108\n",
      "\n",
      "Neighbor size:  3\n",
      "Correct Answers:  11863\n",
      "Wrong Answers:  137\n",
      "Accuracy: %98.62380952380953\n",
      "Precision: %83.73374139250191\n",
      "Recall: %98.32884097035041\n",
      "\n",
      "Neighbor size:  5\n",
      "Correct Answers:  11873\n",
      "Wrong Answers:  127\n",
      "Accuracy: %98.66354166666666\n",
      "Precision: %84.14470145239376\n",
      "Recall: %98.36503694387675\n",
      "\n",
      "Neighbor size:  7\n",
      "Correct Answers:  11875\n",
      "Wrong Answers:  125\n",
      "Accuracy: %98.6962962962963\n",
      "Precision: %84.4871025794841\n",
      "Recall: %98.39318150062876\n",
      "\n",
      "Neighbor size:  9\n",
      "Correct Answers:  11876\n",
      "Wrong Answers:  124\n",
      "Accuracy: %98.72333333333333\n",
      "Precision: %84.77201343008774\n",
      "Recall: %98.41569219162581\n",
      "\n",
      "Selected test data: 2 out of 5\n",
      "With Normalization:\n",
      "\n",
      "Neighbor size:  1\n",
      "Correct Answers:  11725\n",
      "Wrong Answers:  275\n",
      "Accuracy: %98.6310606060606\n",
      "Precision: %82.53343008189074\n",
      "Recall: %98.49084611578427\n",
      "\n",
      "Neighbor size:  3\n",
      "Correct Answers:  11867\n",
      "Wrong Answers:  133\n",
      "Accuracy: %98.65277777777777\n",
      "Precision: %82.75566558750714\n",
      "Recall: %98.53741496598639\n",
      "\n",
      "Neighbor size:  5\n",
      "Correct Answers:  11870\n",
      "Wrong Answers:  130\n",
      "Accuracy: %98.67307692307692\n",
      "Precision: %82.95454545454545\n",
      "Recall: %98.58668341708542\n",
      "\n",
      "Neighbor size:  7\n",
      "Correct Answers:  11874\n",
      "Wrong Answers:  126\n",
      "Accuracy: %98.69285714285714\n",
      "Precision: %83.1543569145012\n",
      "Recall: %98.62907146329606\n",
      "\n",
      "Neighbor size:  9\n",
      "Correct Answers:  11875\n",
      "Wrong Answers:  125\n",
      "Accuracy: %98.71055555555556\n",
      "Precision: %83.33716563194604\n",
      "Recall: %98.66606170598911\n",
      "\n",
      "Without Normalization:\n",
      "\n",
      "Neighbor size:  1\n",
      "Correct Answers:  11722\n",
      "Wrong Answers:  278\n",
      "Accuracy: %98.64635416666667\n",
      "Precision: %82.64062165823056\n",
      "Recall: %98.6049676760803\n",
      "\n",
      "Neighbor size:  3\n",
      "Correct Answers:  11866\n",
      "Wrong Answers:  134\n",
      "Accuracy: %98.66029411764707\n",
      "Precision: %82.78457196613358\n",
      "Recall: %98.63101433031784\n",
      "\n",
      "Neighbor size:  5\n",
      "Correct Answers:  11871\n",
      "Wrong Answers:  129\n",
      "Accuracy: %98.675\n",
      "Precision: %82.9317907316763\n",
      "Recall: %98.66142327762233\n",
      "\n",
      "Neighbor size:  7\n",
      "Correct Answers:  11874\n",
      "Wrong Answers:  126\n",
      "Accuracy: %98.68947368421053\n",
      "Precision: %83.08100609204415\n",
      "Recall: %98.68882997778893\n",
      "\n",
      "Neighbor size:  9\n",
      "Correct Answers:  11873\n",
      "Wrong Answers:  127\n",
      "Accuracy: %98.70208333333333\n",
      "Precision: %83.21092494835897\n",
      "Recall: %98.71349806003676\n",
      "\n",
      "Selected test data: 3 out of 5\n",
      "With Normalization:\n",
      "\n",
      "Neighbor size:  1\n",
      "Correct Answers:  11721\n",
      "Wrong Answers:  279\n",
      "Accuracy: %98.65317460317459\n",
      "Precision: %82.99117115200511\n",
      "Recall: %98.75949367088607\n",
      "\n",
      "Neighbor size:  3\n",
      "Correct Answers:  11865\n",
      "Wrong Answers:  135\n",
      "Accuracy: %98.66325757575758\n",
      "Precision: %83.11576167013766\n",
      "Recall: %98.76267503621439\n",
      "\n",
      "Neighbor size:  5\n",
      "Correct Answers:  11872\n",
      "Wrong Answers:  128\n",
      "Accuracy: %98.675\n",
      "Precision: %83.2498662906598\n",
      "Recall: %98.77697011653397\n",
      "\n",
      "Neighbor size:  7\n",
      "Correct Answers:  11876\n",
      "Wrong Answers:  124\n",
      "Accuracy: %98.68715277777778\n",
      "Precision: %83.38851839761227\n",
      "Recall: %98.79005524861878\n",
      "\n",
      "Neighbor size:  9\n",
      "Correct Answers:  11878\n",
      "Wrong Answers:  122\n",
      "Accuracy: %98.699\n",
      "Precision: %83.52303280157734\n",
      "Recall: %98.80201431221839\n",
      "\n",
      "Without Normalization:\n",
      "\n",
      "Neighbor size:  1\n",
      "Correct Answers:  11722\n",
      "Wrong Answers:  278\n",
      "Accuracy: %98.6599358974359\n",
      "Precision: %83.11326416873874\n",
      "Recall: %98.7671302664425\n",
      "\n",
      "Neighbor size:  3\n",
      "Correct Answers:  11866\n",
      "Wrong Answers:  134\n",
      "Accuracy: %98.66820987654322\n",
      "Precision: %83.2135481206113\n",
      "Recall: %98.76942687650144\n",
      "\n",
      "Neighbor size:  5\n",
      "Correct Answers:  11873\n",
      "Wrong Answers:  127\n",
      "Accuracy: %98.67797619047619\n",
      "Precision: %83.32336880704692\n",
      "Recall: %98.78089117799934\n",
      "\n",
      "Neighbor size:  7\n",
      "Correct Answers:  11876\n",
      "Wrong Answers:  124\n",
      "Accuracy: %98.68793103448276\n",
      "Precision: %83.43539379934526\n",
      "Recall: %98.79155456245155\n",
      "\n",
      "Neighbor size:  9\n",
      "Correct Answers:  11877\n",
      "Wrong Answers:  123\n",
      "Accuracy: %98.6975\n",
      "Precision: %83.54260590931108\n",
      "Recall: %98.80144531594254\n",
      "\n",
      "Selected test data: 4 out of 5\n",
      "With Normalization:\n",
      "\n",
      "Neighbor size:  1\n",
      "Correct Answers:  11749\n",
      "Wrong Answers:  251\n",
      "Accuracy: %98.67204301075269\n",
      "Precision: %83.1382427026226\n",
      "Recall: %98.30063069376314\n",
      "\n",
      "Neighbor size:  3\n",
      "Correct Answers:  11850\n",
      "Wrong Answers:  150\n",
      "Accuracy: %98.67447916666666\n",
      "Precision: %83.20900875053793\n",
      "Recall: %98.27191867852605\n",
      "\n",
      "Neighbor size:  5\n",
      "Correct Answers:  11864\n",
      "Wrong Answers:  136\n",
      "Accuracy: %98.68030303030302\n",
      "Precision: %83.3113135386969\n",
      "Recall: %98.25323929801542\n",
      "\n",
      "Neighbor size:  7\n",
      "Correct Answers:  11866\n",
      "Wrong Answers:  134\n",
      "Accuracy: %98.68627450980392\n",
      "Precision: %83.41318577501856\n",
      "Recall: %98.23571485337361\n",
      "\n",
      "Neighbor size:  9\n",
      "Correct Answers:  11865\n",
      "Wrong Answers:  135\n",
      "Accuracy: %98.69166666666666\n",
      "Precision: %83.50647222677372\n",
      "Recall: %98.21924144310823\n",
      "\n",
      "Without Normalization:\n",
      "\n",
      "Neighbor size:  1\n",
      "Correct Answers:  11746\n",
      "Wrong Answers:  254\n",
      "Accuracy: %98.66921296296296\n",
      "Precision: %83.29734535755112\n",
      "Recall: %98.169978668463\n",
      "\n",
      "Neighbor size:  3\n",
      "Correct Answers:  11850\n",
      "Wrong Answers:  150\n",
      "Accuracy: %98.67139639639639\n",
      "Precision: %83.35083384805435\n",
      "Recall: %98.15252573007965\n",
      "\n",
      "Neighbor size:  5\n",
      "Correct Answers:  11864\n",
      "Wrong Answers:  136\n",
      "Accuracy: %98.6765350877193\n",
      "Precision: %83.4355643754886\n",
      "Recall: %98.1396993810787\n",
      "\n",
      "Neighbor size:  7\n",
      "Correct Answers:  11864\n",
      "Wrong Answers:  136\n",
      "Accuracy: %98.68141025641025\n",
      "Precision: %83.51545334700454\n",
      "Recall: %98.12749552526505\n",
      "\n",
      "Neighbor size:  9\n",
      "Correct Answers:  11864\n",
      "Wrong Answers:  136\n",
      "Accuracy: %98.68604166666667\n",
      "Precision: %83.5918087567475\n",
      "Recall: %98.11599061347637\n",
      "\n",
      "Selected test data: 5 out of 5\n",
      "With Normalization:\n",
      "\n",
      "Neighbor size:  1\n",
      "Correct Answers:  11728\n",
      "Wrong Answers:  271\n",
      "Accuracy: %98.66300541261262\n",
      "Precision: %82.9496090356212\n",
      "Recall: %99.03403565640194\n",
      "\n",
      "Neighbor size:  3\n",
      "Correct Answers:  11857\n",
      "Wrong Answers:  142\n",
      "Accuracy: %98.66666137564037\n",
      "Precision: %82.97917495689083\n",
      "Recall: %99.03745685970301\n",
      "\n",
      "Neighbor size:  5\n",
      "Correct Answers:  11865\n",
      "Wrong Answers:  134\n",
      "Accuracy: %98.67169770366881\n",
      "Precision: %83.02552396762813\n",
      "Recall: %99.04078222662294\n",
      "\n",
      "Neighbor size:  7\n",
      "Correct Answers:  11866\n",
      "Wrong Answers:  133\n",
      "Accuracy: %98.67669452041304\n",
      "Precision: %83.07450263905805\n",
      "Recall: %99.04099228558462\n",
      "\n",
      "Neighbor size:  9\n",
      "Correct Answers:  11868\n",
      "Wrong Answers:  131\n",
      "Accuracy: %98.68183964666339\n",
      "Precision: %83.12346322926757\n",
      "Recall: %99.0441241750762\n",
      "\n",
      "Without Normalization:\n",
      "\n",
      "Neighbor size:  1\n",
      "Correct Answers:  11730\n",
      "Wrong Answers:  269\n",
      "Accuracy: %98.66176081624076\n",
      "Precision: %82.89856477889838\n",
      "Recall: %99.03553740550873\n",
      "\n",
      "Neighbor size:  3\n",
      "Correct Answers:  11856\n",
      "Wrong Answers:  143\n",
      "Accuracy: %98.66487704634632\n",
      "Precision: %82.92410714285714\n",
      "Recall: %99.03857061826432\n",
      "\n",
      "Neighbor size:  5\n",
      "Correct Answers:  11865\n",
      "Wrong Answers:  134\n",
      "Accuracy: %98.6694259642495\n",
      "Precision: %82.96679001140357\n",
      "Recall: %99.04153354632588\n",
      "\n",
      "Neighbor size:  7\n",
      "Correct Answers:  11866\n",
      "Wrong Answers:  133\n",
      "Accuracy: %98.67395929529533\n",
      "Precision: %83.01197946377638\n",
      "Recall: %99.04170750299467\n",
      "\n",
      "Neighbor size:  9\n",
      "Correct Answers:  11867\n",
      "Wrong Answers:  132\n",
      "Accuracy: %98.67847797463291\n",
      "Precision: %83.05505819158459\n",
      "Recall: %99.04449248672165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(12000, 72000, 12000):\n",
    "    cross_code += 1\n",
    "    print(f\"Selected test data: {cross_code} out of 5\")\n",
    "    for l in range(2):\n",
    "        if l == 1:\n",
    "            print(\"Without Normalization:\\n\")\n",
    "            dataset = dict()\n",
    "            test_data = independent_variable[i-12000:i]\n",
    "            train_data = np.concatenate((independent_variable[:i-12000], independent_variable[i:60000]))\n",
    "            for n in range(59999):\n",
    "                dataset[tuple(independent_variable[n])] = dependent_variable[n]\n",
    "        else:\n",
    "            print(\"With Normalization:\\n\")\n",
    "            dataset = dict()\n",
    "            test_data = data_norm[i-12000:i]\n",
    "            train_data = np.concatenate((data_norm[:i-12000], data_norm[i:60000]))\n",
    "            for n in range(59999):\n",
    "                dataset[tuple(data_norm[n])] = dependent_variable[n]\n",
    "        for k in range(1,10,2):\n",
    "            correct_answers = 0\n",
    "            wrong_answers = 0\n",
    "            accuracy = 0\n",
    "            precision = 0\n",
    "            recall = 0\n",
    "            print(\"Neighbor size: \",k)\n",
    "            for testing in test_data:\n",
    "                neighbors = get_neighbors(k, testing)\n",
    "                neighbor_list = []\n",
    "                for neighbor in neighbors:\n",
    "                    neighbor_list.append(dataset[tuple(neighbor)])\n",
    "                majority_vote = max(set(neighbor_list), key = neighbor_list.count)\n",
    "                if majority_vote == dataset[tuple(testing)]:\n",
    "                    correct_answers += 1\n",
    "                    matrix[majority_vote][\"TP\"] += 1\n",
    "                else:\n",
    "                    wrong_answers += 1\n",
    "                    matrix[majority_vote][\"FN\"] += 1\n",
    "                TN_sum = 0\n",
    "                FP_sum = 0\n",
    "                for a in matrix:\n",
    "                    TN_sum += matrix[a][\"TP\"]\n",
    "                    FP_sum += matrix[a][\"FN\"]\n",
    "                TN_sum -= matrix[majority_vote][\"TP\"]\n",
    "                FP_sum -= matrix[majority_vote][\"FN\"]\n",
    "            matrix[majority_vote][\"TN\"] = TN_sum\n",
    "            matrix[majority_vote][\"FP\"] = FP_sum\n",
    "            accuracy += (matrix[majority_vote][\"TP\"] + matrix[majority_vote][\"TN\"]) / (matrix[majority_vote][\"TP\"] + matrix[majority_vote][\"TN\"] + matrix[majority_vote][\"FP\"] + matrix[majority_vote][\"FN\"])\n",
    "            precision += (matrix[majority_vote][\"TP\"]) / (matrix[majority_vote][\"TP\"] + matrix[majority_vote][\"FP\"])\n",
    "            recall += (matrix[majority_vote][\"TP\"]) / (matrix[majority_vote][\"TP\"] + matrix[majority_vote][\"FN\"])\n",
    "            print(\"Correct Answers: \", correct_answers)\n",
    "            print(\"Wrong Answers: \", wrong_answers)\n",
    "            print(f\"Accuracy: %{accuracy*100}\")\n",
    "            print(f\"Precision: %{precision*100}\")\n",
    "            print(f\"Recall: %{recall*100}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the final part, I prepare the datas for the k-nearest neighbor algorithm. I make 5 datas for 5-fold cross validation, 2 for with normalization and without normalization datas and 5 for each k values(1, 3, 5, 7 ,9). In summary I created 5x5x2 = 50 different datas and calculated the correct,wrong answers; accuracy, precision and recall of all of them. For this, I used 3 nested loops: 1 for 5-fold, 1 for normalization, 1 for k values. After the calculations, I created nearest distances for every data and look for corresponding personalities of this data. In distances personalities, I take majority of the personalities(majority_vote) and trying to the guess the personality of selected data with this personality. With correct and wrong answers, I create the confusion matrix and calculate the accuracy, precision and recall with this matrix. In the final, I print summary of the data above. Below is a tabular version of all the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.linkpicture.com/q/AdsÄ±z_60.png"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis\n",
    "As shown in the summary of the datas, the choice of neighbor values ââseriously affects the accuracy, precision and recall percentage. When we compare the values ââwith 1 and 3 neighbors, we see that there is a decrease in the number of wrong answers by almost half. The biggest reason for this is that while only 1 point in 1-neighbor values is taken, the value is obtained by majority voting since there is more than one distance at 3 and beyond. In addition, we can easily say that the number of correct answers increases as the neighbor values increase. In the normalization part, it can be said that there is not much difference between the correct answer values. However, in terms of the processing speed of each data set, it can be said that normalized data sets are processed much faster.\n",
    "Error Analysis\n",
    "we see a number of wrong answers in each dataset, approximately 100 to 300. The biggest reason for this is that the knn algorithm interprets the data set differently, especially for neighbor values with a large difference. We can explain this more easily with an example:\n",
    "\n",
    "\n",
    "O         X      data O        X              O\n",
    "\n",
    "\n",
    "As seen in the example, when we take the value of k = 1, we see that the value of O is dominant, when we take the value of k = 3, the value of X is dominant with the majority vote, and when we take the value of k = 5, we see that the value of O is dominant again. Such errors may vary according to the increase or decrease of neighbor values. Therefore, we cannot say that the largest or smallest neighbor value is the best. However, in line with the results we observed, we can say that the most suitable and efficient data set in terms of speed and correct response is the datasets that are normalized and have 7 neighbor values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "80a8c6aa20e1b36012a14252bd7cd0ba9348d20fd38f235984e5c69476c57bc8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
